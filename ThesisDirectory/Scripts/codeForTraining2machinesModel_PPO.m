%% Step 1: Define the Environment
mdl = 'productionLine_2Machines';
agentBlk = [mdl '/RL Agent'];

% Define observation specifications for 2 machines
obsInfo = rlNumericSpec([5, 1], ...
    'LowerLimit', [0; 0; 0; -1; -1], ...
    'UpperLimit', [1; 1; 1; 1; 1]);
obsInfo.Name = 'Production Line States';

% Define action specifications (2 machines => 4 actions)
actions = dec2bin(0:3, 2) - '0';
actInfo = rlFiniteSetSpec(mat2cell(actions, ones(1, 4), 2));
actInfo.Name = 'MachineActions';

% Create the environment
env = rlSimulinkEnv(mdl, agentBlk, obsInfo, actInfo);

%% Step 2: Define Actor and Critic Networks (same architecture as 5-machine model)
obsDim = prod(obsInfo.Dimension);
actDim = size(actions, 1);

% Actor Network
actorNet = [
    featureInputLayer(obsDim, 'Normalization', 'none', 'Name', 'obsInput')
    fullyConnectedLayer(128, 'Name', 'fc1', ...
        'WeightLearnRateFactor', 1, ...
        'WeightsInitializer', 'glorot')
    reluLayer('Name', 'relu1')
    fullyConnectedLayer(64, 'Name', 'fc2', ...
        'WeightLearnRateFactor', 1, ...
        'WeightsInitializer', 'glorot')
    reluLayer('Name', 'relu2')
    fullyConnectedLayer(actDim, 'Name', 'fcOutput', ...
        'WeightsInitializer', 'zeros')
    softmaxLayer('Name', 'actionProb')];

% Critic Network
criticNet = [
    featureInputLayer(obsDim, 'Normalization', 'none', 'Name', 'obsInput')
    fullyConnectedLayer(128, 'Name', 'fc1', ...
        'WeightLearnRateFactor', 1, ...
        'WeightsInitializer', 'glorot')
    reluLayer('Name', 'relu1')
    fullyConnectedLayer(64, 'Name', 'fc2', ...
        'WeightLearnRateFactor', 1, ...
        'WeightsInitializer', 'glorot')
    reluLayer('Name', 'relu2')
    fullyConnectedLayer(1, 'Name', 'value', ...
        'WeightsInitializer', 'zeros')];

% Create actor and critic representations
actor = rlDiscreteCategoricalActor(actorNet, obsInfo, actInfo);
critic = rlValueFunction(criticNet, obsInfo);

%% Step 3: PPO Agent Options (kept identical for fair comparison)
agentOpts = rlPPOAgentOptions(...
    'ExperienceHorizon', 2048, ...
    'MiniBatchSize', 512, ...
    'ClipFactor', 0.1, ...
    'EntropyLossWeight', 0.02, ...
    'DiscountFactor', 0.995, ...
    'AdvantageEstimateMethod', 'gae', ...
    'GAEFactor', 0.95, ...
    'ActorOptimizerOptions', rlOptimizerOptions('LearnRate', 1e-4, 'GradientThreshold', 1), ...
    'CriticOptimizerOptions', rlOptimizerOptions('LearnRate', 5e-5, 'GradientThreshold', 1));

% Create PPO Agent
agent = rlPPOAgent(actor, critic, agentOpts);

%% Step 4: Training Options (identical to 5-machine training setup)
trainOpts = rlTrainingOptions(...
    'MaxEpisodes', 20000, ...
    'MaxStepsPerEpisode', 2000, ...
    'ScoreAveragingWindowLength', 100, ...
    'Verbose', true, ...
    'Plots', 'training-progress', ...
    'StopTrainingCriteria', 'AverageReward', ...
    'StopTrainingValue', -10);

%% Step 5: Train the Agent
trainingStats = train(agent, env, trainOpts);

% Save the trained agent
save('trainedAgent_2machines.mat', 'agent');
save('trainingStats_2machines.mat', 'trainingStats');

disp('Training completed and agent saved for 2-machine model with consistent settings.');

